{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My summary\n",
    "\n",
    "## Fundamentals of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three kinds of machine learning problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning is just one part of machine learning, and there are other different parts of machine learning. There are three different kinds of machine learning:\n",
    "- Supervised learning\n",
    "- Unsupervised learning\n",
    "- Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training, validation, and test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best practice to split the data into three partsbtraining, validation, and test datasets. The best approach for using the holdout dataset is to:\n",
    "\n",
    "1. Train the algorithm on the training dataset.\n",
    "2. Perform hyper parameter tuning based on the validation dataset.\n",
    "3. Perform the first two steps iteratively until the expected performance is achieved.\n",
    "4. After freezing the algorithm and the hyper parameters, evaluate it on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoid splitting the data into two parts, as it may lead to an information leak. **Training and testing it on the same dataset is a clear no-no as it does not guarantee algorithm generalization**. There are three popular holdout strategies that can be used to split the data\n",
    "into training and validation sets. They are as follows:\n",
    "- Simple holdout validation\n",
    "- K-fold validation\n",
    "- Iterated k-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have looked at different ways to split our datasets to build our evaluation strategy. In most cases, the data that we receive may not be in a format that can be readily used by us for training our algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing for neural networks is a process in which we make the data more suitable for the deep learning algorithms to train on. The following are some of the commonly-used data preprocessing steps:\n",
    "\n",
    "- Vectorization\n",
    "- Normalization\n",
    "- Missing values\n",
    "- Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data comes in various formats such as text, sound, images, and video. The very first thing that needs to be done is to convert the data into PyTorch tensors. In the previous example, we used `torchvision` utility functions to convert **Python Imaging Library (PIL)** images into a Tensor object, though most of the complexity is abstracted away by the PyTorch torchvision libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With recurrent neural networks (RNNs), we will see how text data can be converted into PyTorch tensors. For problems involving structured data, the data is already present in a vectorized format; all we need to do is convert them into PyTorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a common practice to normalize features before passing the data to any machine learning algorithm or deep learning algorithm. It helps in training the algorithms faster and helps in achieving more performance. Normalization is the process in which you represent data belonging to a particular feature in such a way that its mean is zero and standard deviation is one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example of dogs and cats, the classification that we covered in the last chapter, we normalized the data by using the mean and standard deviation of the data available in the `ImageNet` dataset. The reason we chose the `ImageNet` dataset's mean and standard deviation for our example is that we are using the weights of the ResNet model, which was pretrained on ImageNet. It is also a common practice to divide each pixel value by 255 so that all the values fall in the range between zero and one, particularly when you are not using pretrained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is also applied for problems involving structured data. Say we are working on a house price prediction problembthere could be different features that could fall in different scales. For example, distance to the nearest airport and the age of the house are variables or features that could be in different scales. Using them with neural networks as they are could prevent the gradients from converging. In simple words, loss may not go down as expected. So, we should be careful to apply normalization to any kind of data before training on our algorithms. To ensure that the algorithm or model performs better,\n",
    "ensure that the data follows the following characteristics:\n",
    "\n",
    "- **Take small values:** Typically in a range between zero and one.\n",
    "- **Same range:** Ensure all the features are in the same range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are quite common in real-world machine learning problems. From our previous examples of predicting house prices, certain fields for the age of the house could be missing. It is often safe to replace the missing values with a number that may not occur otherwise. The algorithms will be able to identify the pattern. There are other techniques that are available to handle missing values that are more domain-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is the process of using domain knowledge about a particular problem to create new variables or features that can be passed to the model. To understand better, let's look at a sales prediction problem. Say we have information about promotion dates, holidays, competitor's start date, distance from competitor, and sales for a particular day. In the real world, there could be hundreds of features that may be useful in predicting the prices of stores. There could be certain information that could be important in predicting the sales. Some of the important features or derived values are:\n",
    "\n",
    "- Days until the next promotion\n",
    "- Days left before the next holiday\n",
    "- Number of days the competitor's business has been open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There could be many more such features that can be extracted that come from domain knowledge. Extracting these kinds of features for any machine learning algorithm or deep learning algorithm could be quite challenging for the algorithms to perform themselves. For certain domains, particularly in the fields of computer vision and text, modern deep learning algorithms help us in getting away with feature  engineering. Except for these fields, good feature engineering always helps in the following:\n",
    "\n",
    "- The problem can be solved a lot faster with less computational resource.\n",
    "- The deep learning algorithms can learn features without manually engineering them by using huge amounts of data. So, if you are tight on data, then it is good to focus on good feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
